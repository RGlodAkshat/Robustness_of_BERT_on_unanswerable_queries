{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8c9dc3",
   "metadata": {},
   "source": [
    "# Evaluating RoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a9b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========================\n",
    "# 0) Install / upgrade deps\n",
    "# ===========================\n",
    "import sys, subprocess, math, random\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "# def pip_install(pkgs):\n",
    "#     subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs, check=False)\n",
    "\n",
    "# pip_install([\"transformers\", \"huggingface_hub\", \"datasets\"])\n",
    "\n",
    "# ===========================\n",
    "# 1) Imports (after upgrade)\n",
    "# ===========================\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# ===========================\n",
    "# 2) CONFIG\n",
    "# ===========================\n",
    "MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
    "MAX_EXAMPLES = 400\n",
    "MAX_SEQ_LEN = 384\n",
    "DOC_STRIDE = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random.seed(42)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ===========================\n",
    "# 3) LOAD MODEL + TOKENIZER\n",
    "# ===========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2c4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD v2 dev split...\n",
      "Total unanswerable in dev set: 5945\n",
      "Subsampled to 400 examples\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 4) LOAD SQuAD v2 DEV, FILTER UNANSWERABLE\n",
    "# ===========================\n",
    "print(\"Loading SQuAD v2 dev split...\")\n",
    "ds = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "\n",
    "# In SQuAD 2.0, unanswerable questions have empty answers[\"text\"]\n",
    "def is_unanswerable(ex):\n",
    "    return len(ex[\"answers\"][\"text\"]) == 0\n",
    "\n",
    "unans = ds.filter(is_unanswerable)\n",
    "print(f\"Total unanswerable in dev set: {len(unans)}\")\n",
    "\n",
    "if MAX_EXAMPLES is not None:\n",
    "    unans = unans.select(range(min(MAX_EXAMPLES, len(unans))))\n",
    "    print(f\"Subsampled to {len(unans)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00775241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 5) MODEL SCORING: NO-ANSWER PROB\n",
    "# ===========================\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "def get_no_answer_prob(context: str, question: str) -> Tuple[bool, float]:\n",
    "    enc = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits = outputs.start_logits[0]\n",
    "        end_logits = outputs.end_logits[0]\n",
    "\n",
    "    null_score = (start_logits[0] + end_logits[0]).item()\n",
    "    seq_len = start_logits.size(0)\n",
    "    best_non_null = -1e9\n",
    "    for i in range(1, seq_len):\n",
    "        for j in range(i, min(seq_len, i + 15)):\n",
    "            s = start_logits[i].item() + end_logits[j].item()\n",
    "            if s > best_non_null:\n",
    "                best_non_null = s\n",
    "\n",
    "    score_diff = null_score - best_non_null\n",
    "    p_no_ans = sigmoid(score_diff)\n",
    "    pred_no_ans = score_diff > 0.0\n",
    "    return pred_no_ans, p_no_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438cd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 6) PERTURBATIONS\n",
    "# ===========================\n",
    "AFRICAN_PLACE_MAP = {\n",
    "    \"Paris\": \"Lagos\",\n",
    "    \"France\": \"Nigeria\",\n",
    "    \"London\": \"Nairobi\",\n",
    "    \"New York\": \"Accra\",\n",
    "    \"USA\": \"Kenya\",\n",
    "    \"United States\": \"Ghana\",\n",
    "    \"Germany\": \"Ethiopia\",\n",
    "    \"Berlin\": \"Kigali\",\n",
    "}\n",
    "\n",
    "def natural_edit(context: str, question: str) -> Tuple[str, str]:\n",
    "    sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
    "    if len(sentences) <= 1:\n",
    "        return context, question\n",
    "    drop_idx = random.randint(0, len(sentences) - 1)\n",
    "    new_sentences = [s for i, s in enumerate(sentences) if i != drop_idx]\n",
    "    new_context = '. '.join(new_sentences)\n",
    "    if context.endswith('.'):\n",
    "        new_context += '.'\n",
    "    return new_context, question\n",
    "\n",
    "def negation_attack(context: str, question: str) -> Tuple[str, str]:\n",
    "    replacements = [\n",
    "        (\" is \", \" is not \"),\n",
    "        (\" was \", \" was not \"),\n",
    "        (\" are \", \" are not \"),\n",
    "        (\" were \", \" were not \"),\n",
    "        (\" has \", \" has not \"),\n",
    "        (\" have \", \" have not \"),\n",
    "    ]\n",
    "    new_context = context\n",
    "    random.shuffle(replacements)\n",
    "    for old, new in replacements:\n",
    "        if old in new_context:\n",
    "            new_context = new_context.replace(old, new, 1)\n",
    "            break\n",
    "    return new_context, question\n",
    "\n",
    "def entity_swap(context: str, question: str) -> Tuple[str, str]:\n",
    "    new_context = context\n",
    "    new_question = question\n",
    "    for k, v in AFRICAN_PLACE_MAP.items():\n",
    "        if k in new_context:\n",
    "            new_context = new_context.replace(k, v)\n",
    "        if k in new_question:\n",
    "            new_question = new_question.replace(k, v)\n",
    "    return new_context, new_question\n",
    "\n",
    "def paraphrase_stub(context: str, question: str) -> Tuple[str, str]:\n",
    "    sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
    "    if len(sentences) > 1:\n",
    "        random.shuffle(sentences)\n",
    "    new_context = '. '.join(sentences)\n",
    "    if context.endswith('.'):\n",
    "        new_context += '.'\n",
    "    synonym_map = {\n",
    "        \"big\": \"large\",\n",
    "        \"large\": \"big\",\n",
    "        \"small\": \"tiny\",\n",
    "        \"important\": \"significant\",\n",
    "        \"city\": \"urban area\",\n",
    "        \"country\": \"nation\",\n",
    "    }\n",
    "    for k, v in synonym_map.items():\n",
    "        new_context = new_context.replace(f\" {k} \", f\" {v} \")\n",
    "    return new_context, question\n",
    "\n",
    "def identity(context: str, question: str) -> Tuple[str, str]:\n",
    "    return context, question\n",
    "\n",
    "PERTURBATIONS: Dict[str, Callable[[str, str], Tuple[str, str]]] = {\n",
    "    \"original\": identity,\n",
    "    \"natural_edit\": natural_edit,\n",
    "    \"negation\": negation_attack,\n",
    "    \"entity_swap\": entity_swap,\n",
    "    \"paraphrase\": paraphrase_stub,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3515674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7) METRICS + EVALUATION\n",
    "# ===========================\n",
    "def evaluate_perturbation(\n",
    "    dataset,\n",
    "    perturb_name: str,\n",
    "    perturb_fn: Callable[[str, str], Tuple[str, str]],\n",
    "    max_examples: int = None,\n",
    "    num_bins: int = 10,\n",
    ") -> Dict[str, float]:\n",
    "    n = len(dataset) if max_examples is None else min(max_examples, len(dataset))\n",
    "    y_true: List[int] = []\n",
    "    y_pred: List[int] = []\n",
    "    p_hat: List[float] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        ex = dataset[i]\n",
    "        ctx = ex[\"context\"]\n",
    "        q = ex[\"question\"]\n",
    "        ctx_p, q_p = perturb_fn(ctx, q)\n",
    "        pred_no_ans, p_no_ans = get_no_answer_prob(ctx_p, q_p)\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if pred_no_ans else 0)\n",
    "        p_hat.append(float(p_no_ans))\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"[{perturb_name}] {i+1}/{n} examples...\", end=\"\\r\")\n",
    "\n",
    "    print(f\"[{perturb_name}] {n}/{n} examples.           \")\n",
    "    correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n",
    "    accuracy = correct / n if n > 0 else 0.0\n",
    "    hallucination_rate = 1.0 - accuracy\n",
    "    avg_p_no_ans = sum(p_hat) / n if n > 0 else 0.0\n",
    "\n",
    "    bin_bounds = [i / num_bins for i in range(num_bins + 1)]\n",
    "    ece = 0.0\n",
    "    for b in range(num_bins):\n",
    "        lo, hi = bin_bounds[b], bin_bounds[b + 1]\n",
    "        idxs = [\n",
    "            idx for idx, p in enumerate(p_hat)\n",
    "            if (p >= lo and (p < hi or (b == num_bins - 1 and p <= hi)))\n",
    "        ]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        bin_conf = sum(p_hat[k] for k in idxs) / len(idxs)\n",
    "        bin_acc = sum(y_pred[k] == y_true[k] for k in idxs) / len(idxs)\n",
    "        ece += (len(idxs) / n) * abs(bin_acc - bin_conf)\n",
    "\n",
    "    return {\n",
    "        \"accuracy_no_answer\": accuracy,\n",
    "        \"hallucination_rate\": hallucination_rate,\n",
    "        \"avg_p_no_answer\": avg_p_no_ans,\n",
    "        \"ECE\": ece,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b7702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating perturbation: original ===\n",
      "[original] 400/400 examples.           \n",
      "Results for original:\n",
      "  accuracy_no_answer  : 0.8300\n",
      "  hallucination_rate  : 0.1700\n",
      "  avg_p_no_answer     : 0.8339\n",
      "  ECE                 : 0.0323\n",
      "\n",
      "=== Evaluating perturbation: natural_edit ===\n",
      "[natural_edit] 400/400 examples.           \n",
      "Results for natural_edit:\n",
      "  accuracy_no_answer  : 0.8875\n",
      "  hallucination_rate  : 0.1125\n",
      "  avg_p_no_answer     : 0.8791\n",
      "  ECE                 : 0.0295\n",
      "\n",
      "=== Evaluating perturbation: negation ===\n",
      "[negation] 400/400 examples.           \n",
      "Results for negation:\n",
      "  accuracy_no_answer  : 0.8375\n",
      "  hallucination_rate  : 0.1625\n",
      "  avg_p_no_answer     : 0.8324\n",
      "  ECE                 : 0.0336\n",
      "\n",
      "=== Evaluating perturbation: entity_swap ===\n",
      "[entity_swap] 400/400 examples.           \n",
      "Results for entity_swap:\n",
      "  accuracy_no_answer  : 0.8375\n",
      "  hallucination_rate  : 0.1625\n",
      "  avg_p_no_answer     : 0.8366\n",
      "  ECE                 : 0.0313\n",
      "\n",
      "=== Evaluating perturbation: paraphrase ===\n",
      "[paraphrase] 400/400 examples.           \n",
      "Results for paraphrase:\n",
      "  accuracy_no_answer  : 0.8425\n",
      "  hallucination_rate  : 0.1575\n",
      "  avg_p_no_answer     : 0.8412\n",
      "  ECE                 : 0.0289\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1) Run evaluation for all perturbations\n",
    "# ============================================\n",
    "results = {}\n",
    "\n",
    "for name, fn in PERTURBATIONS.items():\n",
    "    print(f\"\\n=== Evaluating perturbation: {name} ===\")\n",
    "    metrics = evaluate_perturbation(unans, name, fn, max_examples=MAX_EXAMPLES)\n",
    "    results[name] = metrics\n",
    "    print(f\"Results for {name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k:20s}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e16bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2) Convert to DataFrame for pretty display\n",
    "# ============================================\n",
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "df = df[[\"accuracy_no_answer\", \"hallucination_rate\", \"avg_p_no_answer\", \"ECE\"]]  # column order\n",
    "df = df.sort_index()\n",
    "df_rounded = df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e0ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================= SUMMARY TABLE =================\n",
      "|              |   accuracy_no_answer |   hallucination_rate |   avg_p_no_answer |    ECE |\n",
      "|--------------|----------------------|----------------------|-------------------|--------|\n",
      "| entity_swap  |               0.8375 |               0.1625 |            0.8366 | 0.0313 |\n",
      "| natural_edit |               0.8875 |               0.1125 |            0.8791 | 0.0295 |\n",
      "| negation     |               0.8375 |               0.1625 |            0.8324 | 0.0336 |\n",
      "| original     |               0.83   |               0.17   |            0.8339 | 0.0323 |\n",
      "| paraphrase   |               0.8425 |               0.1575 |            0.8412 | 0.0289 |\n",
      "\n",
      "Done.\n",
      "\n",
      "Saved results to:\n",
      "  - mrc_robustness_results_20251126_011421.csv\n",
      "  - mrc_robustness_results_20251126_011421.json\n",
      "\n",
      "Saved experiment summary to:\n",
      "  - mrc_robustness_experiment_summary_20251126_011421.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n================= SUMMARY TABLE =================\")\n",
    "print(df_rounded.to_markdown(tablefmt=\"github\"))\n",
    "print(\"\\nDone.\")\n",
    "\n",
    "# ============================================\n",
    "# 3) Save results to disk (CSV + JSON)\n",
    "# ============================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "csv_path = f\"mrc_robustness_results_{timestamp}.csv\"\n",
    "json_path = f\"mrc_robustness_results_{timestamp}.json\"\n",
    "\n",
    "df.to_csv(csv_path, index=True)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved results to:\\n  - {csv_path}\\n  - {json_path}\")\n",
    "\n",
    "# ============================================\n",
    "# 4) Save a human-readable experiment summary\n",
    "# ============================================\n",
    "experiment_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset\": \"SQuAD v2 validation (unanswerable subset)\",\n",
    "    \"max_examples\": MAX_EXAMPLES,\n",
    "    \"num_unanswerable_used\": len(unans),\n",
    "    \"perturbations\": list(PERTURBATIONS.keys()),\n",
    "}\n",
    "\n",
    "summary_lines = []\n",
    "\n",
    "summary_lines.append(\"# MRC Robustness Experiment Summary\\n\")\n",
    "summary_lines.append(f\"**Timestamp:** {timestamp}\")\n",
    "summary_lines.append(f\"**Model:** `{experiment_config['model_name']}`\")\n",
    "summary_lines.append(f\"**Base dataset:** {experiment_config['dataset']}\")\n",
    "summary_lines.append(f\"**Number of unanswerable examples used:** {experiment_config['num_unanswerable_used']}\")\n",
    "summary_lines.append(f\"**Max examples cap:** {experiment_config['max_examples']}\")\n",
    "summary_lines.append(\"**Perturbations applied:**\")\n",
    "for p in experiment_config[\"perturbations\"]:\n",
    "    summary_lines.append(f\"- {p}\")\n",
    "\n",
    "summary_lines.append(\"\\n## Metrics per perturbation\\n\")\n",
    "summary_lines.append(df_rounded.to_markdown(tablefmt=\"github\"))\n",
    "\n",
    "summary_lines.append(\"\\n## Methodology (short description)\\n\")\n",
    "methodology = \"\"\"\n",
    "We evaluate the robustness of a modern reading comprehension (MRC) model on unanswerable\n",
    "questions from the SQuAD v2 validation set. We first filter the dataset to keep only\n",
    "examples where the ground-truth answer is empty (i.e., the question is unanswerable\n",
    "given the context).\n",
    "\n",
    "For each contextâ€“question pair, we generate multiple perturbed versions of the context\n",
    "using the following transformations:\n",
    "1. original: no perturbation, original SQuAD v2 context.\n",
    "2. natural_edit: delete one sentence from the context to simulate a prior revision.\n",
    "3. negation: insert simple negations (e.g., 'is' -> 'is not') into the context.\n",
    "4. entity_swap: replace some common locations with African-origin entities.\n",
    "5. paraphrase: lightly shuffle sentences and swap simple synonyms as a stub for paraphrasing.\n",
    "\n",
    "For each (context, question) pair under each perturbation, we run the QA model and\n",
    "compute the null-vs-span score difference:\n",
    "    score_diff = (start_logits[CLS] + end_logits[CLS]) - best_non_null_span_score.\n",
    "We interpret:\n",
    "    - score_diff > 0  => model predicts \"no answer\".\n",
    "    - score_diff <= 0 => model predicts some answer span (hallucination in this setup).\n",
    "\n",
    "Since all questions are truly unanswerable, the ideal model always predicts \"no answer\".\n",
    "We report:\n",
    "    - accuracy_no_answer: fraction of examples where the model predicts \"no answer\".\n",
    "    - hallucination_rate: fraction where the model predicts a span (1 - accuracy).\n",
    "    - avg_p_no_answer: average sigmoid(score_diff), interpreted as the model's\n",
    "      confidence in \"no answer\".\n",
    "    - ECE: a simple expected calibration error over p(no-answer), using 10 bins.\n",
    "\"\"\"\n",
    "summary_lines.append(textwrap.dedent(methodology).strip() + \"\\n\")\n",
    "\n",
    "summary_path = f\"mrc_robustness_experiment_summary_{timestamp}.txt\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "\n",
    "print(f\"\\nSaved experiment summary to:\\n  - {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830338fd",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2882be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.10/site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, os, math, random, json, textwrap\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "from datetime import datetime\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs, check=False)\n",
    "\n",
    "# Install deps (no-op if already present)\n",
    "pip_install([\"openai\", \"python-dotenv\", \"datasets\", \"pandas\"])\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda244f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT model: gpt-5\n",
      "Reusing existing unans dataset with 400 examples.\n",
      "Reusing existing PERTURBATIONS dict.\n",
      "\n",
      "=== [GPT] Evaluating perturbation: original ===\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, fn \u001b[38;5;129;01min\u001b[39;00m PERTURBATIONS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== [GPT] Evaluating perturbation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 158\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_perturbation_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43munans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_GPT_EXAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     gpt_results[name] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 113\u001b[0m, in \u001b[0;36mevaluate_perturbation_gpt\u001b[0;34m(dataset, perturb_name, perturb_fn, max_examples, num_bins)\u001b[0m\n\u001b[1;32m    110\u001b[0m q \u001b[38;5;241m=\u001b[39m ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m ctx_p, q_p \u001b[38;5;241m=\u001b[39m perturb_fn(ctx, q)\n\u001b[0;32m--> 113\u001b[0m pred_no_ans, p_no_ans \u001b[38;5;241m=\u001b[39m \u001b[43mgpt_predict_no_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y_true\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# all unanswerable\u001b[39;00m\n\u001b[1;32m    116\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_no_ans \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 75\u001b[0m, in \u001b[0;36mgpt_predict_no_answer\u001b[0;34m(context, question)\u001b[0m\n\u001b[1;32m     53\u001b[0m system_msg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     ),\n\u001b[1;32m     64\u001b[0m }\n\u001b[1;32m     66\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     ),\n\u001b[1;32m     73\u001b[0m }\n\u001b[0;32m---> 75\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPT_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m raw \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     83\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1189\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1188\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load API key from .env\n",
    "# --------------------------\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found. Make sure .env has OPENAI_API_KEY=sk-proj-...\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Choose your GPT model here\n",
    "# If you have access to GPT-5, use e.g. \"gpt-5\" or \"gpt-5-mini\".\n",
    "# Otherwise, \"gpt-4.1-mini\" is a solid default.\n",
    "GPT_MODEL = \"gpt-5\"\n",
    "\n",
    "print(f\"Using GPT model: {GPT_MODEL}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) Dataset + Perturbations (re-use if already defined)\n",
    "# ============================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "MAX_GPT_EXAMPLES = 200  # lower than 400 to save cost; bump if you want\n",
    "\n",
    "# Try to reuse `unans` if defined; otherwise load from scratch\n",
    "unans  # type: ignore[name-defined]\n",
    "print(f\"Reusing existing unans dataset with {len(unans)} examples.\")\n",
    "\n",
    "\n",
    "# Try to reuse PERTURBATIONS if defined; otherwise recreate simple ones\n",
    "PERTURBATIONS  # type: ignore[name-defined]\n",
    "print(\"Reusing existing PERTURBATIONS dict.\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# 2) GPT-based \"no-answer\" prediction\n",
    "# ============================================================\n",
    "\n",
    "def gpt_predict_no_answer(context: str, question: str) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Ask GPT to perform RC with an explicit \"NO_ANSWER\" option.\n",
    "\n",
    "    Protocol:\n",
    "      - If the answer is present in the context, GPT should reply with a short answer span.\n",
    "      - If not, GPT must reply with **exactly** 'NO_ANSWER'.\n",
    "\n",
    "    We then:\n",
    "      - pred_no_ans = True if cleaned output == NO_ANSWER\n",
    "      - p_no_ans = 1.0 if pred_no_ans else 0.0 (binary confidence proxy)\n",
    "    \"\"\"\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a strict reading comprehension model. \"\n",
    "            \"You MUST follow these rules exactly:\\n\"\n",
    "            \"1. You are given a context and a question.\\n\"\n",
    "            \"2. If the question CAN be answered using ONLY the context, \"\n",
    "            \"   reply with a short answer span copied exactly from the context.\\n\"\n",
    "            \"3. If the question CANNOT be answered from the context, reply with \"\n",
    "            \"   EXACTLY the token: NO_ANSWER (all caps, no punctuation, nothing else).\\n\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\n\"\n",
    "            \"Remember: if the answer is not present in the context, reply with NO_ANSWER.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[system_msg, user_msg],\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=32,\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "    cleaned = raw.strip().lower().replace(\".\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "    # We accept a few variants just in case: \"NO_ANSWER\", \"NO ANSWER\"\n",
    "    pred_no_ans = cleaned in {\"no_answer\"}\n",
    "\n",
    "    p_no_ans = 1.0 if pred_no_ans else 0.0  # binary confidence proxy\n",
    "    return pred_no_ans, p_no_ans\n",
    "\n",
    "# ============================================================\n",
    "# 3) Evaluation (same metrics as RoBERTa version)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_perturbation_gpt(\n",
    "    dataset,\n",
    "    perturb_name: str,\n",
    "    perturb_fn: Callable[[str, str], Tuple[str, str]],\n",
    "    max_examples: int = None,\n",
    "    num_bins: int = 10,\n",
    ") -> Dict[str, float]:\n",
    "    n = len(dataset) if max_examples is None else min(max_examples, len(dataset))\n",
    "    y_true: List[int] = []\n",
    "    y_pred: List[int] = []\n",
    "    p_hat: List[float] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        ex = dataset[i]\n",
    "        ctx = ex[\"context\"]\n",
    "        q = ex[\"question\"]\n",
    "\n",
    "        ctx_p, q_p = perturb_fn(ctx, q)\n",
    "        pred_no_ans, p_no_ans = gpt_predict_no_answer(ctx_p, q_p)\n",
    "\n",
    "        y_true.append(1)  # all unanswerable\n",
    "        y_pred.append(1 if pred_no_ans else 0)\n",
    "        p_hat.append(float(p_no_ans))\n",
    "\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"[{perturb_name}] Processed {i+1}/{n} examples...\", end=\"\\r\")\n",
    "\n",
    "    print(f\"[{perturb_name}] Processed {n}/{n} examples.           \")\n",
    "\n",
    "    correct = sum(yt == yp for yt, yp in zip(y_true, y_pred))\n",
    "    accuracy = correct / n if n > 0 else 0.0\n",
    "    hallucination_rate = 1.0 - accuracy\n",
    "    avg_p_no_ans = sum(p_hat) / n if n > 0 else 0.0\n",
    "\n",
    "    # Simple ECE over p_hat (which is 0/1 here, so still meaningful)\n",
    "    bin_bounds = [i / num_bins for i in range(num_bins + 1)]\n",
    "    ece = 0.0\n",
    "    for b in range(num_bins):\n",
    "        lo, hi = bin_bounds[b], bin_bounds[b + 1]\n",
    "        idxs = [\n",
    "            idx for idx, p in enumerate(p_hat)\n",
    "            if (p >= lo and (p < hi or (b == num_bins - 1 and p <= hi)))\n",
    "        ]\n",
    "        if not idxs:\n",
    "            continue\n",
    "        bin_conf = sum(p_hat[k] for k in idxs) / len(idxs)\n",
    "        bin_acc = sum(y_pred[k] == y_true[k] for k in idxs) / len(idxs)\n",
    "        ece += (len(idxs) / n) * abs(bin_acc - bin_conf)\n",
    "\n",
    "    return {\n",
    "        \"accuracy_no_answer\": accuracy,\n",
    "        \"hallucination_rate\": hallucination_rate,\n",
    "        \"avg_p_no_answer\": avg_p_no_ans,\n",
    "        \"ECE\": ece,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 4) Run evaluation for all perturbations\n",
    "# ============================================================\n",
    "\n",
    "gpt_results = {}\n",
    "for name, fn in PERTURBATIONS.items():\n",
    "    print(f\"\\n=== [GPT] Evaluating perturbation: {name} ===\")\n",
    "    metrics = evaluate_perturbation_gpt(unans, name, fn, max_examples=MAX_GPT_EXAMPLES)\n",
    "    gpt_results[name] = metrics\n",
    "    print(f\"Results for {name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k:20s}: {v:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Pretty summary + save artifacts\n",
    "# ============================================================\n",
    "\n",
    "df_gpt = pd.DataFrame.from_dict(gpt_results, orient=\"index\")\n",
    "df_gpt = df_gpt[[\"accuracy_no_answer\", \"hallucination_rate\", \"avg_p_no_answer\", \"ECE\"]]\n",
    "df_gpt = df_gpt.sort_index()\n",
    "df_gpt_rounded = df_gpt.round(4)\n",
    "\n",
    "print(\"\\n\\n================= GPT SUMMARY TABLE =================\")\n",
    "print(df_gpt_rounded.to_markdown(tablefmt=\"github\"))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "csv_path = f\"gpt_mrc_robustness_results_{timestamp}.csv\"\n",
    "json_path = f\"gpt_mrc_robustness_results_{timestamp}.json\"\n",
    "\n",
    "df_gpt.to_csv(csv_path, index=True)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(gpt_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved GPT results to:\\n  - {csv_path}\\n  - {json_path}\")\n",
    "\n",
    "# ------------------ Summary text file ------------------\n",
    "summary_lines = []\n",
    "summary_lines.append(\"# GPT MRC Robustness Experiment Summary\\n\")\n",
    "summary_lines.append(f\"**Timestamp:** {timestamp}\")\n",
    "summary_lines.append(f\"**GPT Model:** `{GPT_MODEL}`\")\n",
    "summary_lines.append(\"**Base dataset:** SQuAD v2 validation (unanswerable subset)\")\n",
    "summary_lines.append(f\"**Number of unanswerable examples used:** {len(unans)}\")\n",
    "summary_lines.append(f\"**Max examples cap (GPT):** {MAX_GPT_EXAMPLES}\")\n",
    "summary_lines.append(\"**Perturbations applied:**\")\n",
    "for p in PERTURBATIONS.keys():\n",
    "    summary_lines.append(f\"- {p}\")\n",
    "\n",
    "summary_lines.append(\"\\n## Metrics per perturbation\\n\")\n",
    "summary_lines.append(df_gpt_rounded.to_markdown(tablefmt=\"github\"))\n",
    "\n",
    "methodology = f\"\"\"\n",
    "We evaluate the robustness of a large language model (`{GPT_MODEL}`) on\n",
    "unanswerable questions from the SQuAD v2 validation set. As in the RoBERTa\n",
    "baseline experiment, we first filter the dataset to keep only examples where\n",
    "the ground-truth answer is empty, meaning the question is unanswerable given\n",
    "the context.\n",
    "\n",
    "For each contextâ€“question pair, we generate multiple perturbed versions of the\n",
    "context using the same transformations as before:\n",
    "1. original: no perturbation, original SQuAD v2 context.\n",
    "2. natural_edit: delete one sentence from the context to simulate a prior revision.\n",
    "3. negation: insert simple negations (e.g., 'is' -> 'is not') into the context.\n",
    "4. entity_swap: replace common locations with African-origin entities.\n",
    "5. paraphrase: lightly shuffle sentences and swap simple synonyms as a stub for paraphrasing.\n",
    "\n",
    "Instead of span extraction logits, we use GPT as a generative QA system with\n",
    "an explicit protocol. The model receives the context and question, and is\n",
    "instructed to:\n",
    "  - reply with a short answer span *copied from the context* if the question\n",
    "    is answerable, and\n",
    "  - reply with the exact token 'NO_ANSWER' if the question is not answerable\n",
    "    from the context.\n",
    "\n",
    "Since in this experiment all questions are truly unanswerable, the ideal model\n",
    "should always output 'NO_ANSWER'. For each example, we post-process the model\n",
    "output:\n",
    "  - If the cleaned output equals 'NO_ANSWER', we treat this as a correct\n",
    "    'no-answer' prediction.\n",
    "  - Otherwise, we treat it as a hallucinated answer span.\n",
    "\n",
    "We report:\n",
    "  - accuracy_no_answer: fraction of examples where GPT outputs 'NO_ANSWER'.\n",
    "  - hallucination_rate: fraction where GPT outputs some other text\n",
    "    (1 - accuracy).\n",
    "  - avg_p_no_answer: average predicted probability of 'no-answer'; here we\n",
    "    approximate this by 1.0 for 'NO_ANSWER' predictions and 0.0 otherwise.\n",
    "  - ECE: a simple expected calibration error over p(no-answer), using 10 bins.\n",
    "\n",
    "These metrics are directly comparable in spirit to the RoBERTa baseline and\n",
    "allow us to analyze how a modern GPT model behaves on unanswerable queries and\n",
    "under the same perturbations.\n",
    "\"\"\"\n",
    "summary_lines.append(\"\\n## Methodology (short description)\\n\")\n",
    "summary_lines.append(textwrap.dedent(methodology).strip() + \"\\n\")\n",
    "\n",
    "summary_path = f\"gpt_mrc_robustness_experiment_summary_{timestamp}.txt\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "\n",
    "print(f\"\\nSaved GPT experiment summary to:\\n  - {summary_path}\\n\")\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
